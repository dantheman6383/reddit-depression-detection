{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantheman6383/reddit-depression-detection/blob/main/Reddit_Depression_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoBxKQ_OVl-j",
        "outputId": "076a3dfd-4c88-4ccf-b715-4d395b87be26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting happiestfuntokenizing\n",
            "  Downloading happiestfuntokenizing-0.0.7.tar.gz (6.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: happiestfuntokenizing\n",
            "  Building wheel for happiestfuntokenizing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for happiestfuntokenizing: filename=happiestfuntokenizing-0.0.7-py3-none-any.whl size=6711 sha256=7423955a6308d2a08c0fa5ce1feb13bdb3b46f6a49f367a27a2bd3a15f8b00e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/c9/4d/310f0c60855eb7b428558f29d93cf464dbb64c1b8628753395\n",
            "Successfully built happiestfuntokenizing\n",
            "Installing collected packages: happiestfuntokenizing\n",
            "Successfully installed happiestfuntokenizing-0.0.7\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import string\n",
        "!pip install happiestfuntokenizing\n",
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = '/content/drive/MyDrive/student.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMOTL7mV9T9"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "outputs": [],
      "source": [
        "def load():\n",
        "  \"\"\"Load pickles\"\"\"\n",
        "  # open pickle file\n",
        "  with open(FILEPATH, 'rb') as f:\n",
        "    # load pickle\n",
        "    data = pd.read_pickle(f)\n",
        "  return data\n",
        "\n",
        "data = load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQvhy4AISd9Z"
      },
      "outputs": [],
      "source": [
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpw9kJiras4B",
        "outputId": "2d2bf4fe-9302-47b4-f3c0-d0cbaf29cc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "315         BuddermanTheAmazing\n",
            "651         WildernessExploring\n",
            "730        NeighborhoodPizzaGuy\n",
            "1354                    xDEDANx\n",
            "1598                baby_kicked\n",
            "                   ...         \n",
            "1968023          QueenGeraldina\n",
            "1969250              NewMe43893\n",
            "1969342               briarjohn\n",
            "1969582       sweetmotherofodin\n",
            "1969647             Mustang2006\n",
            "Name: author, Length: 4369, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def dataset_generation():\n",
        "  \"\"\"Build control and symptom datasets\"\"\"\n",
        "  datasets = []\n",
        "  # use dict to associate each symptom to the subreddits that comprise them\n",
        "  symptom_to_sub = {}\n",
        "\n",
        "  # get one dataset for each symptom using subreddit column value\n",
        "  anger = data.loc[data['subreddit'] == \"Anger\"]\n",
        "  datasets.append(anger)\n",
        "  symptom_to_sub[\"Anger\"] = [\"Anger\"]\n",
        "\n",
        "  anhedonia = data.loc[(data['subreddit'] == \"anhedonia\") | (data['subreddit'] == \"DeadBedrooms\")]\n",
        "  datasets.append(anger)\n",
        "  symptom_to_sub[\"Anhedonia\"] = [\"anhedonia\", \"DeadBedrooms\"]\n",
        "\n",
        "  anxiety = data.loc[(data['subreddit'] == \"Anxiety\") | (data['subreddit'] == \"AnxietyDepression\") | (data['subreddit'] == \"HealthAnxiety\") | (data['subreddit'] == \"PanicAttack\")]\n",
        "  datasets.append(anxiety)\n",
        "  symptom_to_sub[\"Anxiety\"] = [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"]\n",
        "\n",
        "  # excluded symptoms should still be in depression dataset, but should not get their own symptom dataset\n",
        "  concentration_deficit = data.loc[(data['subreddit'] == \"DecisionMaking\") | (data['subreddit'] == \"shouldi\")]\n",
        "\n",
        "  disordered_eating = data.loc[(data['subreddit'] == \"bingeeating\") | (data['subreddit'] == \"BingeEatingDisorder\") | (data['subreddit'] == \"EatingDisorders\") | (data['subreddit'] == \"eating_disorders\") | (data['subreddit'] == \"EDAnonymous\")]\n",
        "  datasets.append(anxiety)\n",
        "  symptom_to_sub[\"Disordered eating\"] = [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"]\n",
        "\n",
        "  fatigue = data.loc[(data['subreddit'] == \"chronicfatigue\") | (data['subreddit'] == \"Fatigue\")]\n",
        "\n",
        "  loneliness = data.loc[(data['subreddit'] == \"ForeverAlone\") | (data['subreddit'] == \"lonely\")]\n",
        "  datasets.append(anxiety)\n",
        "  symptom_to_sub[\"Loneliness\"] = [\"ForeverAlone\", \"lonely\"]\n",
        "\n",
        "  sad_mood = data.loc[(data['subreddit'] == \"cry\") | (data['subreddit'] == \"grief\") | (data['subreddit'] == \"sad\") | (data['subreddit'] == \"Sadness\")]\n",
        "  datasets.append(sad_mood)\n",
        "  symptom_to_sub[\"Sad mood\"] = [\"cry\", \"grief\", \"sad\", \"Sadness\"]\n",
        "\n",
        "  self_loathing = data.loc[(data['subreddit'] == \"AvPD\") | (data['subreddit'] == \"SelfHate\") | (data['subreddit'] == \"selfhelp\") | (data['subreddit'] == \"socialanxiety\") | (data['subreddit'] == \"whatsbotheringyou\")]\n",
        "  datasets.append(self_loathing)\n",
        "  symptom_to_sub[\"Self-loathing\"] = [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"]\n",
        "\n",
        "  sleep_problem = data.loc[(data['subreddit'] == \"insomnia\") | (data['subreddit'] == \"sleep\")]\n",
        "  datasets.append(sleep_problem)\n",
        "  symptom_to_sub[\"Sleep problem\"] = [\"insomnia\", \"sleep\"]\n",
        "\n",
        "  somatic_complaint = data.loc[(data['subreddit'] == \"cfs\") | (data['subreddit'] == \"ChronicPain\") | (data['subreddit'] == \"Constipation\") | (data['subreddit'] == \"EssentialTremor\") | (data['subreddit'] == \"headaches\") | (data['subreddit'] == \"ibs\") | (data['subreddit'] == \"tinnitus\")]\n",
        "  datasets.append(somatic_complaint)\n",
        "  symptom_to_sub[\"Somatic complaint\"] = [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"]\n",
        "\n",
        "  suicide = data.loc[(data['subreddit'] == \"AdultSelfHarm\") | (data['subreddit'] == \"selfharm\") | (data['subreddit'] == \"SuicideWatch\")]\n",
        "\n",
        "  worthlessness = data.loc[(data['subreddit'] == \"Guilt\") | (data['subreddit'] == \"Pessimism\") | (data['subreddit'] == \"selfhelp\") | (data['subreddit'] == \"whatsbotheringyou\")]\n",
        "  datasets.append(worthlessness)\n",
        "  symptom_to_sub[\"Worthlessness\"] = [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
        "\n",
        "  # depression dataset\n",
        "  datasets.insert(0, pd.concat([anger, anhedonia, anxiety, disordered_eating, loneliness, sad_mood, self_loathing, sleep_problem, somatic_complaint, worthlessness, concentration_deficit, suicide, fatigue]))\n",
        "\n",
        "  # map each author to index depression posting\n",
        "  author_to_index = {}\n",
        "  for _, post in datasets[0].iterrows():\n",
        "    # if we've seen the author before, update their index post if the current post is earlier than the post already stored\n",
        "    if post['author'] in author_to_index:\n",
        "      author_to_index[post['author']] = min(post['created_utc'], author_to_index[post['author']])\n",
        "    # haven't seen this author; store current post as their index post thus far\n",
        "    else:\n",
        "      author_to_index[post['author']] = post['created_utc']\n",
        "\n",
        "  # Filter non-depression posts to include only those older than 180 days from the author's earliest depression posting\n",
        "  c = data.loc[~data['subreddit'].isin(depression_subreddits)]\n",
        "  c = c.loc[(c['author'].isin(author_to_index.keys())) & (c['author'].map(author_to_index) - c['created_utc'] >= 15552000)]\n",
        "\n",
        "  datasets.insert(0, c)\n",
        "  return datasets, symptom_to_sub\n",
        "\n",
        "datasets, symptom_to_sub = dataset_generation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWGVUju_WxuP"
      },
      "outputs": [],
      "source": [
        "def tokenize():\n",
        "  dataset_tokens = []\n",
        "  tokenizer = Tokenizer()\n",
        "\n",
        "  for dataset in datasets:\n",
        "    # take the 'text' column, remove punctuation, and then tokenize (which automatically lowercases), and cast to list\n",
        "    dataset_tokens.append(list(dataset['text'].apply(lambda x: tokenizer.tokenize(x.translate(str.maketrans('', '', string.punctuation))))))\n",
        "  return dataset_tokens\n",
        "\n",
        "dataset_tokens = tokenize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3j9z7UuW3eG"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def stop_words():\n",
        "  control = dataset_tokens[0]\n",
        "\n",
        "  # use Counter to get token frequencies\n",
        "  counter = Counter([token for row in control for token in row])\n",
        "\n",
        "  # get top 100 words\n",
        "  stop_words = counter.most_common(100)\n",
        "  return stop_words\n",
        "\n",
        "stop_words = stop_words()\n",
        "# filter stop words from control and depression tokens\n",
        "dataset_tokens[0] = [[token for token in row if token not in stop_words] for row in dataset_tokens[0]]\n",
        "dataset_tokens[1] = [[token for token in row if token not in stop_words] for row in dataset_tokens[1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4I37U1SXAEZ"
      },
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaMulticore\n",
        "from gensim import corpora\n",
        "\n",
        "=vocab = set()\n",
        "corpus = dataset_tokens[0] + dataset_tokens[1]\n",
        "\n",
        "# go through control + depression and build vocab\n",
        "for post in corpus:\n",
        "  for token in post:\n",
        "    vocab.add(token)\n",
        "\n",
        "# map id to word for use in LdaMulticore\n",
        "counter = Counter(token for post in corpus for token in post)\n",
        "temp = [x[0] for x in counter.most_common(len(vocab))]\n",
        "idx2word = dict(enumerate(temp))\n",
        "\n",
        "# use corpora.Dictionary to make a mapping based off corpus that can then be turned into BOW for LdaMulticore\n",
        "dictionary = corpora.Dictionary(corpus)\n",
        "corpus = [dictionary.doc2bow(post) for post in corpus]\n",
        "\n",
        "# train LdaMulticore on control + depression posts\n",
        "lda = LdaMulticore(corpus, num_topics=200, id2word=idx2word, minimum_probability=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxnePVF6VuuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2aac373-97aa-4a1b-f0f9-f68370137ad9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('suggested', 0.2816144),\n",
              " ('recovery', 0.24950284),\n",
              " ('aside', 0.058846075),\n",
              " ('scapegoat', 0.04413658),\n",
              " ('the', 0.03955097),\n",
              " ('things', 0.026652614),\n",
              " ('â€™', 0.024239523),\n",
              " ('nature', 0.022204313),\n",
              " ('out', 0.013725635),\n",
              " ('her', 0.013718098)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# get topic distribution for each post in control + depression set\n",
        "topics = [lda.get_document_topics(post) for post in corpus]\n",
        "# example of top 10 words associated with first topic\n",
        "lda.show_topic(0, topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build feature matrix\n",
        "X = np.zeros((len(corpus), 200))\n",
        "\n",
        "for i in range(len(topics)):\n",
        "  for topic, prob in topics[i]:\n",
        "    # each matrix element is the probability of that topic for the post\n",
        "    X[i, topic] = prob"
      ],
      "metadata": {
        "id": "iCxnZ_9YUp27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build Y (labels matrix)\n",
        "def build_labels():\n",
        "  labels = []\n",
        "\n",
        "  for symptom in [\"Anger\", \"Anhedonia\", \"Anxiety\", \"Disordered eating\", \"Loneliness\", \"Sad mood\", \"Self-loathing\", \"Sleep problem\", \"Somatic complaint\", \"Worthlessness\"]:\n",
        "    # store labels for a symptom in 1-d matrix\n",
        "    Y = np.zeros(len(corpus))\n",
        "    i = 0\n",
        "\n",
        "    for _, row in (pd.concat([datasets[0], datasets[1]])).iterrows():\n",
        "      # if the post's subreddit is associated with the symptom, label it 1, otherwise the label stays 0\n",
        "      if row.subreddit in symptom_to_sub[symptom]:\n",
        "        Y[i] = 1\n",
        "      i = i + 1\n",
        "    labels.append(Y)\n",
        "  return labels\n",
        "Y = build_labels()"
      ],
      "metadata": {
        "id": "ffNIwfTBV75r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0-97hsVXNkF"
      },
      "source": [
        "## RoBERTa Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blx1SWVMXYDp"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
        "model = RobertaModel.from_pretrained('distilroberta-base')\n",
        "\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "# get the text from each post in control + depression dataset for tokenizing\n",
        "d = pd.concat([datasets[0], datasets[1]])\n",
        "text = [post['text'] for _, post in d.iterrows()]\n",
        "\n",
        "embeddings = []\n",
        "for post in text:\n",
        "  # tokenize post text\n",
        "  inputs = tokenizer(post, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "  inputs.to(\"cuda\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # use ** to automatically assign input_ids and attention_mask from BatchEncoding from tokenizer; also get hidden states for next line\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "  # get hidden representation from 5th layer according to handout\n",
        "  hidden_states = outputs.hidden_states[5]\n",
        "\n",
        "  embedding = hidden_states.squeeze(0)\n",
        "  avg_embedding = embedding.mean(dim=0)\n",
        "\n",
        "  # store embedding on cpu and convert to numpy array\n",
        "  embeddings.append(avg_embedding.cpu().numpy())\n",
        "\n",
        "embeddings = np.array(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWxuF2jXtwi"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koTBPhcDXujb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a303cc-8ad0-40ae-a535-51b67cab6d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA\n",
            "Train [0.99148455 0.98859688 0.99160357 0.99546661 0.99341526]\n",
            "Test [0.82961361 0.83996481 0.83610435 0.81059423 0.81556339]\n",
            "Roberta\n",
            "Train [0.99999346 0.99999916 0.99999978 0.99999894 0.99999952]\n",
            "Test [0.84275583 0.82967878 0.83604131 0.82275119 0.86708919]\n",
            "LDA\n",
            "Train [0.96978483 0.96948293 0.96851002 0.96977235 0.97041406]\n",
            "Test [0.93248721 0.93293243 0.92638122 0.93221944 0.92695327]\n",
            "Roberta\n",
            "Train [0.99996361 0.99996705 0.99997112 0.99996414 0.99997894]\n",
            "Test [0.9565865  0.95489873 0.95402909 0.95589525 0.953619  ]\n",
            "LDA\n",
            "Train [0.85097463 0.84930155 0.84816622 0.84965519 0.84956431]\n",
            "Test [0.80946351 0.80546596 0.81206463 0.80964726 0.80051919]\n",
            "Roberta\n",
            "Train [0.998358   0.99822777 0.99831063 0.99833068 0.99831906]\n",
            "Test [0.85812061 0.86245369 0.8597909  0.85712015 0.86529438]\n",
            "LDA\n",
            "Train [0.94397735 0.94457728 0.94632828 0.94780974 0.95186543]\n",
            "Test [0.81522398 0.83316798 0.80939285 0.81502377 0.80447113]\n",
            "Roberta\n",
            "Train [0.99990092 0.9999263  0.9998928  0.9999668  0.99981033]\n",
            "Test [0.90462548 0.9141049  0.91352341 0.888185   0.91256274]\n",
            "LDA\n",
            "Train [0.8669696  0.86704164 0.86610107 0.86634496 0.86433366]\n",
            "Test [0.80800224 0.79672123 0.80878452 0.80277277 0.80626578]\n",
            "Roberta\n",
            "Train [0.99866663 0.9987034  0.99848818 0.99876183 0.99894514]\n",
            "Test [0.88413443 0.88039857 0.87431393 0.88045938 0.87962922]\n",
            "LDA\n",
            "Train [0.90408518 0.89729846 0.90620436 0.89383514 0.89313692]\n",
            "Test [0.75491581 0.73409143 0.74642932 0.73769624 0.74509939]\n",
            "Roberta\n",
            "Train [0.98144093 0.98288741 0.97932273 0.97398176 0.98076692]\n",
            "Test [0.85206115 0.83089458 0.82426564 0.83478455 0.83814024]\n",
            "LDA\n",
            "Train [0.83768249 0.83887492 0.83699894 0.83558028 0.83548397]\n",
            "Test [0.75391033 0.75506783 0.75078633 0.75660337 0.74623355]\n",
            "Roberta\n",
            "Train [0.99463984 0.99457728 0.99529688 0.99477446 0.99469552]\n",
            "Test [0.86766422 0.86729679 0.8558049  0.8639967  0.85673306]\n",
            "LDA\n",
            "Train [0.99112888 0.98984014 0.99046782 0.98988446 0.99016787]\n",
            "Test [0.95266686 0.96129161 0.96950093 0.96678083 0.96074646]\n",
            "Roberta\n",
            "Train [0.99992712 0.99991633 0.99989138 0.99990852 0.9999082 ]\n",
            "Test [0.94969832 0.95377735 0.95433618 0.95423049 0.95475842]\n",
            "LDA\n",
            "Train [0.89177691 0.8916219  0.89416726 0.89186879 0.89124754]\n",
            "Test [0.84700743 0.84620657 0.84008974 0.84046337 0.8442772 ]\n",
            "Roberta\n",
            "Train [0.9980613  0.99751078 0.99789057 0.99829787 0.99815243]\n",
            "Test [0.9105463  0.91345676 0.91722058 0.90587233 0.91011836]\n",
            "LDA\n",
            "Train [0.89517359 0.89184542 0.8910432  0.88791686 0.8899771 ]\n",
            "Test [0.81782123 0.83763921 0.8487176  0.81493003 0.83176698]\n",
            "Roberta\n",
            "Train [0.96388529 0.97167601 0.96510115 0.97192432 0.96149483]\n",
            "Test [0.90441557 0.91537128 0.91732903 0.92196117 0.92399784]\n"
          ]
        }
      ],
      "source": [
        "def main(X, y):\n",
        "  \"\"\"\n",
        "  Here's the basic structure of the main block! It should run\n",
        "  5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n",
        "  performance.\n",
        "  \"\"\"\n",
        "  # evaluate each symptom vs control\n",
        "  for symptom in y:\n",
        "    # classifier and kfold work together to cross_validate the feature matrix X based on the symptom labels with AUC scoring\n",
        "    rf_classifier = RandomForestClassifier(n_estimators = 85, max_depth=15, n_jobs=-1)\n",
        "    cv = KFold(n_splits=5, shuffle=True)\n",
        "    results = cross_validate(rf_classifier, X=X, y=symptom, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "\n",
        "    print(\"LDA\")\n",
        "    print(\"Train\", results['train_score'])\n",
        "    print(\"Test\", results['test_score'])\n",
        "\n",
        "    # do same thing but with embeddings instead of feature matrix\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=50, max_depth=15, n_jobs=-1)\n",
        "    cv = KFold(n_splits=5, shuffle=True)\n",
        "    results = cross_validate(rf_classifier, X=embeddings, y=symptom, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "\n",
        "    print(\"Roberta\")\n",
        "    print(\"Train\", results['train_score'])\n",
        "    print(\"Test\", results['test_score'])\n",
        "\n",
        "main(X, Y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}